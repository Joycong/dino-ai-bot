# 📌 experiment/good-memory 브랜치 변경 요약

이 브랜치는 `main` 브랜치 기반에서 다음과 같은 기능 향상을 실험적으로 적용한 버전입니다.

---

## 🔄 주요 변경 사항

### 1. 🎯 좋은 메모리 전략 (Good Memory Strategy) 도입

- 학습 중 에피소드마다 가장 높은 장애물 회피 수(`max_obstacles`)를 기준으로,  
  **절반 이상 성공한 경험만을 `good_memory`에 저장**하여 추가 학습에 사용함
- 기준 공식: `threshold = max_obstacles - int(max_obstacles / 2)`
- `good_memory`는 일반 메모리와 함께 혼합해 `replay()` 학습에 사용되어,  
  **더 가치 있는 경험 위주의 학습**을 유도함

> ✅ 참고: `good_memory`는 현재 디스크에 저장되지 않고, 학습 중 메모리에만 유지됩니다.

---

### 2. 📈 보상 구조 개선

- 에피소드 중 넘은 장애물 수(`episode_obstacles`)를 기반으로,  
  **에피소드 종료 시 누적 보상**을 한 번에 계산합니다.
- 계산 공식: `1 + 2 + ... + N` → `bonus = N * (N + 1) // 2`
- 마지막에 `-10`을 적용하여 **충돌 시 패널티**를 반영합니다.

> 예: 장애물 5개를 넘고 충돌한 경우 → `bonus = 15`, `total_reward = 15 - 10 = 5`

- 이 방식은 **연속적인 회피를 장려하고**,  
  **장애물을 더 많이 피할수록 높은 보상을 주어 장기 생존을 유도**합니다.

---

### 3. 🔽 탐험률(ε) 감소 방식 개선

- 기존: `epsilon *= 0.998` (고정된 감소율)
- 개선 후:
    - 최대 장애물 수 갱신 시 → `epsilon *= 0.9`
    - 동일 수준 유지 시 → `epsilon *= 0.95`
    - 퇴보 or 정체 시 → `epsilon *= 0.99`

> → 학습 성과에 따라 **탐험률 감소 속도를 유연하게 조정**하여,  
> 안정성과 최적화 속도 향상을 함께 도모합니다.

---

### 4. 🧠 DQN 학습 흐름 유지 + 메모리 최적화

- 기존과 동일하게:
    - 가장 최근 모델 및 메모리를 자동 로딩
    - `models/`, `memory/`, `logs/` 폴더 자동 생성 및 관리
    - `training_log.csv`에 보상, 탐험률, 장애물 수 기록
- 변화된 점:
    - `good_memory` 도입 → 학습 대상 데이터를 전략적으로 필터링
    - `replay()`에서 일부 비율(`good_ratio`)로 good memory 사용

---

## 📁 저장 구조 요약

| 항목 | 경로 | 설명 |
|------|------|------|
| 모델 저장 | `models/dqn_model_ep{N}.pth` | 에피소드 단위 PyTorch 모델 |
| 일반 메모리 | `memory/replay_ep{N}.pkl` | 경험 리플레이 메모리 |
| 좋은 메모리 | (런타임 메모리) | 파일로 저장되지 않음 |
| 로그 파일 | `logs/training_log.csv` | 학습 기록 (보상, ε 등) 저장 |

---

## 🧪 브랜치 목적

- 가치 높은 경험 위주의 강화학습 실험
- 장기 생존 중심의 보상 설계 실험
- 탐험률 제어 전략이 학습 성능에 미치는 영향 분석

---

## 🔁 병합 계획

성능 개선 및 안정성 확인 후, `main` 브랜치에 병합될 수 있습니다.
